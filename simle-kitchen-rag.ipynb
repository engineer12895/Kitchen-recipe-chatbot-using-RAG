{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9448211,"sourceType":"datasetVersion","datasetId":5742520}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install some basic library:","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain_community pypdf\n!pip install langchain-groq\n!pip install sentence_transformers\n!pip install faiss-gpu\n!pip install langgraph\n!pip install --upgrade pydantic","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now Put Your API key:","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\nos.environ['LANGCHAIN_PROJECT'] = 'advanced-rag'\nos.environ['LANGCHAIN_API_KEY'] = \"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\"\nos.environ['GROQ_API_KEY'] = \"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\"\nos.environ['TAVILY_API_KEY'] = \"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\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import Libraries:","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceBgeEmbeddings\nfrom langchain_groq import ChatGroq\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.prompts import ChatPromptTemplate\nfrom pydantic import BaseModel, Field\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.load import dumps, loads\nfrom typing_extensions import TypedDict\nfrom operator import itemgetter\nfrom typing import List\nfrom langchain_core.runnables import RunnablePassthrough\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the documents:","metadata":{}},{"cell_type":"code","source":"loader_1 = PyPDFLoader('/kaggle/input/kitchen-dataset/cook_book.pdf')\ncook_pdf =loader_1.load()\n\nloader_2 = PyPDFLoader('/kaggle/input/kitchen-dataset/List_of_all_recipe.pdf')\ncook_pdf.extend(loader_2.load())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cook_pdf","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split the Documents:","metadata":{}},{"cell_type":"code","source":"text_spliter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\nsplits = text_spliter.split_documents(cook_pdf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"splits","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Embedding the documents:","metadata":{}},{"cell_type":"code","source":"model_name = \"BAAI/bge-small-en\"\nmodel_kwargs = {\"device\": \"cpu\"}\nencode_kwargs = {\"normalize_embeddings\": True}\nhf_embeddings = HuggingFaceBgeEmbeddings(\n    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n)\nvectorstore = FAISS.from_documents(documents=splits, \n                                    embedding=hf_embeddings)\n\nretriever = vectorstore.as_retriever()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LLMs","metadata":{}},{"cell_type":"markdown","source":"## Generate multiple query prompt: ","metadata":{}},{"cell_type":"code","source":"# Multi Query: Different Perspectives\ntemplate = \"\"\"You are an AI language model assistant. Your task is to generate seven \ndifferent versions of the given user question to retrieve relevant documents from a vector \ndatabase. By generating multiple perspectives on the user question, your goal is to help\nthe user overcome some of the limitations of the distance-based similarity search. \nProvide these alternative questions separated by newlines. Original question: {question}\"\"\"\nprompt_perspectives = ChatPromptTemplate.from_template(template)\n\ngenerate_queries = (\n    prompt_perspectives \n    | ChatGroq(temperature=0) \n    | StrOutputParser() \n    | (lambda x: x.split(\"\\n\"))\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_queries.invoke(\"how to cook chicken?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_unique_union(documents: list[list]):\n    \"\"\" Unique union of retrieved docs \"\"\"\n    # Flatten list of lists, and convert each Document to string\n    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n    # Get unique documents\n    unique_docs = list(set(flattened_docs))\n    # Return\n    return [loads(doc) for doc in unique_docs]\n\n# Retrieve\nquestion = \"how to cook chicken?\"\nretrieval_chain = generate_queries | retriever.map() | get_unique_union\ndocs = retrieval_chain.invoke({\"question\":question})\nlen(docs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Retrival gradder:","metadata":{}},{"cell_type":"code","source":"# I used the class for data modeling \nclass GradeRetrival(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n\nllm = ChatGroq(temperature=0)\nstructured_llm_grader = llm.with_structured_output(GradeRetrival)\n\n# Prompt \nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\nretrieval_grader = grade_prompt | structured_llm_grader\nquestion = \"how to cook chicken?\"\ndocs = retriever.get_relevant_documents(question)\ndoc_txt = docs\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate answer:","metadata":{}},{"cell_type":"code","source":"# RAG\ntemplate = \"\"\"Answer the following question based on this context:\n\n{context}\n\nQuestion: {question}\n\nPlease provide the answer in the following format:\n1. List of all ingredients required.\n2. Step-by-step method for cooking.\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nllm = ChatGroq(temperature=0)\n\nfinal_rag_chain = ({\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} | prompt| llm| StrOutputParser()\n)\n# \nans_generation = final_rag_chain.invoke({\"question\":question})\nprint(ans_generation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## fallback prompt: ","metadata":{}},{"cell_type":"code","source":"fallback_prompt = ChatPromptTemplate.from_template(\n    (\n        \"You are a friendly medical assistant created by NHVAI.\\n\"\n        \"Do not respond to queries that are not related to health.\\n\"\n        \"If a query is not related to health, acknowledge your limitations.\\n\"\n        \"Provide concise responses to only medically-related queries.\\n\\n\"\n        \"Current conversations:\\n\\n{chat_history}\\n\\n\"\n        \"human: {query}\"\n    )\n)\n\nfallback_chain = (\n    {\n        \"chat_history\": lambda x: \"\\n\".join(\n            [\n                (\n                    f\"human: {msg.content}\"\n                    if isinstance(msg, HumanMessage)\n                    else f\"AI: {msg.content}\"\n                )\n                for msg in x[\"chat_history\"]\n            ]\n        ),\n        \"query\": itemgetter(\"query\") ,\n    }\n    | fallback_prompt\n    | llm\n    | StrOutputParser()\n)\n\nfallback_chain.invoke(\n    {\n        \"query\": \"Hello\",\n        \"chat_history\": [],\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hallucination Grader:","metadata":{}},{"cell_type":"code","source":"# I used the class for data modeling \nclass HallucinationGrader(BaseModel):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n    binary_score: str = Field(description= \"Answer is grounded in the fact, 'yes' or 'No'\")\n\n# so here I use ChatGroq LLm\nllm = ChatGroq(temperature=0)\nstructure_llm_grader = llm.with_structured_output(HallucinationGrader)\n\n# Prompt\nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\nhallucination_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\n\nhallucination_grader = hallucination_prompt | structured_llm_grader\nhallucination_grader.invoke({\"documents\": docs, \"generation\": ans_generation})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Answer grader","metadata":{}},{"cell_type":"code","source":"class GradeAnswer(BaseModel):\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\n\n    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n        \n# LLM with function call\nllm = ChatGroq(temperature=0)\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\n\n# Prompt\nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\nanswer_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\nanswer_grader = answer_prompt | structured_llm_grader\nanswer_grader.invoke({\"question\": question,\"generation\": ans_generation})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LLM\nllm = ChatGroq(temperature=0)\n\n# Prompt\nsystem = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n     for vectorstore retrieval. Look at the input and try to reason about the underlying sematic intent / meaning. Just say the question. I dont need any explanation just question is enough\"\"\"\nre_write_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n    ]\n)\n\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\nquestion_rewriter.invoke({\"question\": question})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Graph state","metadata":{}},{"cell_type":"code","source":"class GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        documents: list of documents\n    \"\"\"\n    question : str\n    ans_generation : str\n    documents : List[str]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Node:","metadata":{}},{"cell_type":"code","source":"def retrieve(state):\n    \"\"\"\n    Retrieve documents\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    # Retrieval\n    documents = retriever.get_relevant_documents(question)\n    return {\"documents\": documents, \"question\": question}\n\ndef generate(state):\n    \"\"\"\n    Generate answer\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains LLM generation\n    \"\"\"\n    print(\"---GENERATE---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # RAG generation\n    ans_generation = final_rag_chain.invoke({\"context\": documents, \"question\": question})\n    return {\"documents\": documents, \"question\": question, \"generation\": ans_generation}\n\n\ndef fallback_node(state: dict):\n    \"\"\"\n    Fallback to this node when there is no tool call\n    \"\"\"\n    query = state[\"query\"]\n    chat_history = state[\"chat_history\"]\n    ans_generation = fallback_chain.invoke({\"query\": query, \"chat_history\": chat_history})\n    return {\"generation\": ans_generation}\n\n\ndef grade_documents(state):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates documents key with only filtered relevant documents\n    \"\"\"\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Score each doc\n    filtered_docs = []\n    for d in documents:\n        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            continue\n    return {\"documents\": filtered_docs, \"question\": question}\n\ndef transform_query(state):\n    \"\"\"\n    Transform the query to produce a better question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates question key with a re-phrased question\n    \"\"\"\n\n    print(\"---TRANSFORM QUERY---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Re-write question\n    better_question = question_rewriter.invoke({\"question\": question})\n    return {\"documents\": documents, \"question\": better_question}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Edges:","metadata":{}},{"cell_type":"code","source":"def decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or re-generate a question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    question = state[\"question\"]\n    filtered_documents = state[\"documents\"]\n\n    if not filtered_documents:\n        # All documents have been filtered check_relevance\n        # We will re-generate a new query\n        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n        return \"transform_query\"\n    else:\n        # We have relevant documents, so generate answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate\"\n\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determines whether the generation is grounded in the document and answers question.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    ans_generation = state[\"generation\"]\n\n    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": ans_generation})\n    grade = score.binary_score\n\n    # Check hallucination\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check question-answering\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": question,\"generation\": ans_generation})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return \"useful\"\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"not useful\"\n    else:\n        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langgraph.graph import END, StateGraph\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"retrieve\", retrieve)\nworkflow.add_node(\"grade_documents\", grade_documents)\nworkflow.add_node(\"fallback_node\", fallback_node)\nworkflow.add_node(\"generate\", generate) \nworkflow.add_node(\"transform_query\", transform_query)\n\n# build graph\nworkflow.set_entry_point(\"retrieve\")\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate,\n    {\n        \"transform_query\": \"transform_query\",\n        \"generate\": \"generate\",\n        \"fallback\": \"fallback_node\",\n    },\n)\nworkflow.add_edge(\"transform_query\", \"retrieve\")\nworkflow.add_conditional_edges(\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not supported\": \"generate\",\n        \"useful\": END,\n        \"not useful\": \"transform_query\",\n    },\n)\nworkflow.add_edge(\"fallback_node\", END)\n\n# Compile\napp = workflow.compile()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pprint import pprint\n\n# Run\ninputs = {\"question\": \"Chapli Kababs?\"}\n\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        # Node\n        pprint(f\"Node '{key}':\")\n        # Optional: print full state at each node\n        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n    pprint(\"\\n---\\n\")\n\n# Final generation\npprint(value[\"generation\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}